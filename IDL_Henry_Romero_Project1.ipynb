{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ga0ZSCZdofY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Image Classification with Cats vs Dogs dataset\n",
    "# HPE DSI 312 – Introduction to Deep Learning – Spring 2025\n",
    "# Project 1\n",
    "\n",
    "# Load and Prepare Dataset\n",
    "# The cats_vs_dogs dataset has 25,000 high resolution images and will need to work with a 0,1 format.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "imVl1Byg81GI",
    "outputId": "b8804709-3aed-42ec-ca1f-ca4d811598c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 855ms/step - accuracy: 0.9234 - loss: 0.1701 - val_accuracy: 0.9776 - val_loss: 0.0662\n",
      "Epoch 2/3\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 805ms/step - accuracy: 0.9777 - loss: 0.0594 - val_accuracy: 0.9785 - val_loss: 0.0590\n",
      "Epoch 3/3\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 732ms/step - accuracy: 0.9816 - loss: 0.0528 - val_accuracy: 0.9785 - val_loss: 0.0580\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 697ms/step - accuracy: 0.9788 - loss: 0.0588\n",
      "Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "IMG_SIZE = 160  # All images will be resized to 160x160 for high resolution pictures.\n",
    "\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs', #specified the datset from tfds that gets loaded in\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'], #Training/Validation/Test\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")\n",
    "\n",
    "# Preprocess for resize and normalize\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE #automatically tunes\n",
    "\n",
    "# Applied the preprocessing to my batches\n",
    "# Added .cache() as it helped with memory and run times\n",
    "train_batches = raw_train.map(preprocess, num_parallel_calls=AUTOTUNE).cache().shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "validation_batches = raw_validation.map(preprocess, num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "test_batches = raw_test.map(preprocess, num_parallel_calls=AUTOTUNE).cache().batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "#  Model setup\n",
    "# Used transfer learning with base model to handle this type of dataset and help with performance.\n",
    "# Switched from the EfficientNetB0 model to MobileNetV2 for faster processing as the previous model took 10 minutes per epoch.\n",
    "# Noticed it still had a high accuracy rate at a fraction of the time.\n",
    "# Classification head to pair with the model and the data's layers.\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the Model\n",
    "# Used 3 epochs after adjusting the parameters for higher accuracy\n",
    "EPOCHS = 3\n",
    "history = model.fit(train_batches,\n",
    "                    validation_data=validation_batches,\n",
    "                    epochs=EPOCHS)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_batches)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4r4ds4MeaCQ"
   },
   "source": [
    "#  Model setup\n",
    "Used transfer learning with base model to handle this type of dataset and help with performance. Switched from the EfficientNetB0 model to MobileNetV2 for faster processing as the previous model took 10 minutes per epoch. Noticed it still had a high accuracy rate at a fraction of the time. Classification head to pair with the model and the data's layers.\n",
    "# Compile the Model\n",
    "Used the adam optimizer as it doesn't require much tuning, loss was for binary responses, and accuracy helps for evaluation.\n",
    "\n",
    "# Evaluate the Model\n",
    "With 98 percent accuracy, I chose just accuracy to keep things simple, but recall and and precision could help for more details.A confusion matrix could be a great visual for this binary response.\n",
    "\n",
    "# Intended uses and potential limitations:\n",
    "This model can classify cat vs. dog images with decent accuracy of .98 percent. It was a decent sized dataset with limited epoch, but potential for overfitting unless base_model is fine-tuned.\n",
    "\n",
    "# Improvements\n",
    "Could invololve adding more data augmentation like flips or rotations, fine tuning top layers, and use a more diverse dataset. I don't think the model is overfitting as validation and training accuracy is so close, if trained longer or used a different base, it could lead to overfitting as stated before."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
